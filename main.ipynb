import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load and preprocess your text data
# Replace this with your own text data
with open('data.txt', 'r', encoding='utf-8') as file:
    text = file.read()
vocab = sorted(set(text))
char_to_idx = {char: idx for idx, char in enumerate(vocab)}
idx_to_char = {idx: char for char, idx in char_to_idx.items()}
num_chars = len(vocab)

# Create input sequences and target characters
sequence_length = 100
step = 3
sequences = []
next_chars = []

for i in range(0, len(text) - sequence_length, step):
    sequences.append(text[i:i + sequence_length])
    next_chars.append(text[i + sequence_length])

# Vectorize the data
X = np.zeros((len(sequences), sequence_length, num_chars), dtype=np.bool)
y = np.zeros((len(sequences), num_chars), dtype=np.bool)

for i, sequence in enumerate(sequences):
    for t, char in enumerate(sequence):
        X[i, t, char_to_idx[char]] = 1
    y[i, char_to_idx[next_chars[i]]] = 1

# Build the LSTM model
model = Sequential()
model.add(LSTM(128, input_shape=(sequence_length, num_chars)))
model.add(Dense(num_chars, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

# Train the model
h = model.fit(X, y, epochs=20, verbose=1, batch_size=32, validation_split=0.2)

# Generate text using the trained model
start_idx = np.random.randint(0, len(text) - sequence_length - 1)
seed_sequence = text[start_idx:start_idx + sequence_length]
generated_text = seed_sequence
print(generated_text)
for _ in range(400):
    x_pred = np.zeros((1, sequence_length, num_chars))
    for t, char in enumerate(seed_sequence):
        x_pred[0, t, char_to_idx[char]] = 1.

    preds = model.predict(x_pred, verbose=0)[0]
    next_char_idx = np.random.choice(range(num_chars), p=preds)
    next_char = idx_to_char[next_char_idx]

    generated_text += next_char
    seed_sequence = seed_sequence[1:] + next_char

print(generated_text)
